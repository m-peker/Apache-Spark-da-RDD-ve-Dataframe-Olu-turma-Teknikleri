{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD ve Dataframe oluşturma teknikleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ön işlemler: SparkSession ve SparkContext Oluşturma </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SparkSession</b> ve <b>SparkContext</b>; Spark için bir başlangıç noktasıdır.\n",
    "<p>SparkSession içerisindeki <b><i>master</i></b>; çalışılacak ortamı ifade eder. Bir küme üzerinde çalışılıyorsa; \n",
    "bu küme burada belirtilir. Örneğin YARN gibi. Örneğimizde bağımsız modda çalıştığımız için <b><i>local[x]</i></b> ifadesini kullandık. x bir tamsayı değeri olmalı ve 0'dan büyük olmalıdır; bu, RDD, DataFrame ve Dataset kullanılırken kaç bölüm oluşturması gerektiğini gösterir. İdeal olarak, x değeri sahip olduğunuz CPU çekirdeği sayısı olmalıdır.</p>\n",
    "<p><b><i>appName()</i></b> - Uygulama adınızı ayarlamak için kullanılır.</p>\n",
    "<p><b><i>getOrCreate()</i></b> - Bu, zaten varsa bir SparkSession nesnesi döndürür, yoksa yeni bir tane oluşturur.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[8]\") \\\n",
    ".appName(\"Python-Spark-Veri-Seti-İslemleri\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1 - parallelize( ) metodu </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Örnek 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "daylist_as_rdd = sc.parallelize([\"Pazartesi\",\"Salı\",\"Çarşamba\",\"Perşembe\",\"Cuma\",\"Cumartesi\",\"Pazar\"]).map(lambda x: Row(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = daylist_as_rdd.toDF(['Günler']) # toDF ile rdd'yi DataFrame formatına çeviriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   Günler|\n",
      "+---------+\n",
      "|Pazartesi|\n",
      "|     Salı|\n",
      "| Çarşamba|\n",
      "| Perşembe|\n",
      "|     Cuma|\n",
      "|Cumartesi|\n",
      "|    Pazar|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Örnek 2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = sc.parallelize([(1, 2, 3, 'a b c'),\n",
    "(4, 5, 6, 'd e f'),\n",
    "(7, 8, 9, 'g h i')]).toDF(['Sütun-1', 'Sütun-2', 'Sütun-3','Sütun-4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|Sütun-1|Sütun-2|Sütun-3|Sütun-4|\n",
      "+-------+-------+-------+-------+\n",
      "|      1|      2|      3|  a b c|\n",
      "|      4|      5|      6|  d e f|\n",
      "|      7|      8|      9|  g h i|\n",
      "+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Not</font>: ToDF fonksiyonu kullanılmadan elde edilen veri RDD verisidir. Bu veriyi listelemek için bir action fonksiyonu olarak collect metodu kullanılmaktadır. Örnek 1'e tekrar bakalım. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "daylist_as_rdd = sc.parallelize([\"Pazartesi\",\"Salı\",\"Çarşamba\",\"Perşembe\",\"Cuma\",\"Cumartesi\",\"Pazar\"]).map(lambda x: Row(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row('Pazartesi')>,\n",
       " <Row('Salı')>,\n",
       " <Row('Çarşamba')>,\n",
       " <Row('Perşembe')>,\n",
       " <Row('Cuma')>,\n",
       " <Row('Cumartesi')>,\n",
       " <Row('Pazar')>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daylist_as_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2- createDataFrame( ) metodu</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[8]\") \\\n",
    ".appName(\"Python-Spark-Veri-Seti-İslemleri\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "df_3 = spark.createDataFrame([\n",
    "('1', 'Ali', '70000', '1'),\n",
    "('2', 'Fatma', '80000', '2'),\n",
    "('3', 'Ayşe', '60000', '2'),\n",
    "('4', 'Mehmet', '90000', '1')],\n",
    "['Id', 'İsim', 'Maaş','Bölüm-Id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n",
      "| Id|  İsim| Maaş|Bölüm-Id|\n",
      "+---+------+-----+--------+\n",
      "|  1|   Ali|70000|       1|\n",
      "|  2| Fatma|80000|       2|\n",
      "|  3|  Ayşe|60000|       2|\n",
      "|  4|Mehmet|90000|       1|\n",
      "+---+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3- read() ve load () metotları</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dosyadan Okuma</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-----+\n",
      "|      data|venda|estoque|preco|\n",
      "+----------+-----+-------+-----+\n",
      "|2014-01-01|    0|   4972| 1.29|\n",
      "|2014-01-02|   70|   4902| 1.29|\n",
      "|2014-01-03|   59|   4843| 1.29|\n",
      "|2014-01-04|   93|   4750| 1.29|\n",
      "|2014-01-05|   96|   4654| 1.29|\n",
      "+----------+-----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- data: string (nullable = true)\n",
      " |-- venda: integer (nullable = true)\n",
      " |-- estoque: integer (nullable = true)\n",
      " |-- preco: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[8]\") \\\n",
    ".appName(\"Python-Spark-Veri-Seti-İslemleri\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "options(header='true', \\\n",
    "inferschema='true').\\\n",
    "load(\"./mock_kaggle.csv\", header=True) # Veri seti Kaggle'dan indirilmiştir.\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Veritabanından Okuma</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jardrv = \"./postgresql-42.2.14.jar\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', jardrv).getOrCreate()\n",
    "url = 'jdbc:postgresql://url:port/database_name'\n",
    "properties = {'user': 'username', 'password': 'password'}\n",
    "df = spark.read.jdbc(url=url, table='public.\"table_name\"', properties=properties)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
